<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
"http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html>

<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8"/>
<meta name="description" content="description"/>
<meta name="keywords" content="keywords"/> 
<meta name="author" content="author"/> 
<link rel="stylesheet" type="text/css" href="default.css" media="screen"/>
<title>MaxSAT Evaluation 2021</title>
</head>

<body>

<div class="outer-container">

<div class="inner-container">

<div class="header">
                
<div class="title">

<span class="sitename"><a href="index.html"><font color="#00b386">MaxSAT Evaluation</font> <font color="#00ddaa">2021</font></a></span>
<div class="slogan">Affiliated with <a href="https://static-webs.doc.iiia.csic.es/sat2021/">SAT 2021</a>  &nbsp;  &middot; &nbsp;  July 5-9 &nbsp;  &middot; &nbsp; Barcelona, Spain<br><br></div>
    </div>
</div>


<div class="main">		

<div class="content">

<p></p>

<h1>Incomplete Track Score</h1>

The current scoring scheme used in the incomplete track is a relative score---each solver’s score depends on the other solvers that participate. The score gives a reasonable idea of solver ranking in the evaluation, but it is difficult to apply when solver writers run their own tests.
<br>
<br>
Hence, this year we propose to use the best known cost for each instance, which is the cost of the currently best known solution for the instance. The score a solver obtains will be computed by the same expression, but this time the “cost of the best solution for i found by any solver + 1” will become the “benchmark cost + 1”:
<br>
<br>
Previous Incomplete score for a solver s on instance i: 
<br>&nbsp&nbsp&nbsp&#8721<sub>i &#8712
    instances solved by some solver</sub>&nbsp(cost of
    best solution for i found by any solver + 1)<br>&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp / (cost of solution for i
    found by solver + 1)</li>
<br>
<br>
New incomplete score: 
<br>&nbsp&nbsp&nbsp&#8721<sub>i &#8712
    instances solved by some solver</sub>&nbsp(<b>cost of best known cost for i</b> + 1)<br>&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp / (cost of solution for i
    found by solver + 1)</li>
<br>
<br>
<b>If a solver in the evaluation finds a solution with better cost than the current best known cost, the best known cost will be updated to that value and used in the evaluation scoring.</b>
<br>
<br>
The best known solution costs will be published along with the evaluation results. The current best known costs for all instances will be posted to the website
<a href="http://www.cs.toronto.edu/maxsat-lib/">http://www.cs.toronto.edu/maxsat-lib/</a>
<br>
<br>

In future work we encourage researchers to use the following methodology in their experiments with incomplete solvers. 
<ul>
<li>Use the most recent best known cost obtained from <a href="http://www.cs.toronto.edu/maxsat-lib/">maxsat-lib</a> in computing scores for each solver.</li>
<li>If some solver in your experiments finds a better solution than the posted best known cost, then:
  <ul>
<li>Use that better solution cost in your scores so that each score is never greater than one.</li> 
<li>Please inform the organizers of the evaluation so that the best known cost can be updated. (Please include a v-line verifying your new better solution). </li>
<li>Include in the description of your results a discussion of any new best solutions found. </li>
</li>
</ul>

</div>


  <!-- NAVIGATION -->

  
<div class="navigation">

<h2><b>Important Dates</b></h2>
<ul>
<li><a href="submission.html">Submissions: <span class="right"><br>June 15<br></span></a></li><br>
<li><a href="rankings.html">Results: <span class="right">at SAT'21</span></a></li>
</ul>

<h2><b>Calls</b></h2>
<ul>
  <li><a href="call-for-benchmarks.html">Call for Benchmarks</a></li>
  <li><a href="call-for-solvers.html">Call for Solvers</a></li>
  </ul>

<h2><b>Participation</b></h2>
<ul>
  <li><a href="tracks.html">Tracks</a></li>
  <li><a href="rules.html">Rules</a></li>
  <li><a href="execution.html">Execution Environment</a></li>
<li><a href="submission.html">Submissions</a></li>
</ul>

<h2><b>Results</b></h2>

<ul>
  <li><a href="rankings.html">Rankings and Data</a></li>
  <li><a href="benchmarks.html">Benchmarks</a></li>
  <li><a href="descriptions.html">Solvers</a></li>
  </ul>
  

<h2><b>Further Information</b></h2>
<ul>
  <li><a href="organization.html">Organizers</a></li>
<li><a href="history.html">History</a></li>
</ul>

</div>



<div class="clearer">&nbsp;</div>




<div class="footer">

<span class="right">


</span>

<div class="clearer"></div>

</div>




</div>

</div>

</body>

</html>
